# stub_chat.py

import json
import os
from openai import OpenAI
import logging

log = logging.getLogger(__name__)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 1) Carga global de la plantilla payload.json
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
HERE = os.path.dirname(__file__)
PAYLOAD_PATH = os.path.join(HERE, "payload.json")

with open(PAYLOAD_PATH, encoding="utf-8") as f:
    base_payload = json.load(f)
# Ahora base_payload tiene keys como "model", "temperature", "max_tokens", etc.
# y su lista original de mensajes (p. ej. el system prompt base).

# Inicializa el cliente de LM Studio con timeout
client = OpenAI(
    base_url="http://127.0.0.1:1234/v1",
    api_key="lm-studio",
    timeout=120.0  # 120 segundos timeout para consultas complejas
)


def chat(user_message: str, chat_id: str, history: list) -> str:
    """
    Genera la respuesta del bot usando TODO el historial previo,
    y respetando el 'model' y par√°metros definidos en payload.json.
    Incluye contextos diarios y por usuario desde la DB.
    """

    # 1) Reconstruye el payload a enviar (hacemos copia para no mutar la global)
    payload = base_payload.copy()
    payload["messages"] = list(base_payload.get("messages", []))

    # Detectar el modelo y su l√≠mite de tokens
    model_token_limits = {
        # Modelos locales LM Studio t√≠picos
        "phi-4": 4096,
        "meta-llama-3.1-8b-instruct": 4096,
        "nemotron-mini-4b-instruct": 4096,
        # Modelos OpenAI
        "gpt-3.5-turbo": 4096,
        "gpt-3.5-turbo-16k": 16384,
        "gpt-4": 8192,
        "gpt-4-32k": 32768,
        "gpt-4-turbo": 128000,
        # Puedes agregar m√°s modelos y sus l√≠mites aqu√≠
    }
    model_name = str(payload.get("model", "phi-4"))
    # Buscar l√≠mite por nombre exacto o por prefijo
    max_model_tokens = 4096  # default
    for k, v in model_token_limits.items():
        if model_name == k or model_name.startswith(k):
            max_model_tokens = v
            break

    # Asegurar par√°metros seguros por si vienen fuera de rango en payload.json
    try:
        mt = int(payload.get("max_tokens", 2048) or 2048)
        payload["max_tokens"] = max(16, min(mt, max_model_tokens))
    except Exception:
        payload["max_tokens"] = min(2048, max_model_tokens)

    # 2) Inyecta perfil/estrategia y Docs locales como system prompts previos al historial
    try:
        import chat_sessions as cs
        profile = cs.get_profile(chat_id)
        active_strategy = cs.get_active_strategy(chat_id)
        pre_systems = []
        
        # Primero agregar el perfil del bot/empresa
        def _read_text(fp: str) -> str:
            try:
                with open(fp, 'r', encoding='utf-8') as f:
                    return f.read()
            except UnicodeDecodeError:
                try:
                    with open(fp, 'r', encoding='cp1252', errors='ignore') as f:
                        return f.read()
                except Exception:
                    return ""
            except Exception:
                return ""

        def _load_chat_specific_context(chat_id: str) -> dict:
            """
            Carga el contexto espec√≠fico para un chat desde contextos/{chat_id}/
            Retorna dict con 'perfil', 'contexto' y 'objetivo' espec√≠ficos del usuario
            """
            # Consider both folders: contextos/{chat_id} and contextos/chat_{chat_id}
            candidates = [
                os.path.join(HERE, 'contextos', chat_id),
                os.path.join(HERE, 'contextos', f'chat_{chat_id}')
            ]
            result = {'perfil': '', 'contexto': '', 'objetivo': ''}

            log.debug(f"üîç Buscando contexto para chat_id: {chat_id}")
            for contextos_dir in candidates:
                log.debug(f"üîç Directorio candidato: {contextos_dir} -> existe={os.path.exists(contextos_dir)}")
                if not os.path.exists(contextos_dir):
                    continue

                # Cargar perfil espec√≠fico del usuario
                perfil_path = os.path.join(contextos_dir, 'perfil.txt')
                if os.path.exists(perfil_path) and not result['perfil']:
                    result['perfil'] = _read_text(perfil_path)
                    log.info(f"‚úÖ Cargado perfil del usuario: {len(result['perfil'])} caracteres")

                # Cargar contexto espec√≠fico del usuario
                contexto_path = os.path.join(contextos_dir, 'contexto.txt')
                if os.path.exists(contexto_path) and not result['contexto']:
                    result['contexto'] = _read_text(contexto_path)
                    log.info(f"‚úÖ Cargado contexto del usuario: {len(result['contexto'])} caracteres")

                # Cargar objetivo espec√≠fico del usuario
                objetivo_path = os.path.join(contextos_dir, 'objetivo.txt')
                if os.path.exists(objetivo_path) and not result['objetivo']:
                    result['objetivo'] = _read_text(objetivo_path)
                    log.info(f"‚úÖ Cargado objetivo del usuario: {len(result['objetivo'])} caracteres")

            return result

        docs_dir = os.path.join(HERE, 'Docs')
        
        # Cargar archivos globales del sistema
        try:
            perfil_fp = os.path.join(docs_dir, 'Perfil.txt')
            ej_fp = os.path.join(docs_dir, 'ejemplo_chat.txt')
            ult_fp = os.path.join(docs_dir, 'Ultimo_contexto.txt')
            
            perfil_global_txt = _read_text(perfil_fp) if os.path.exists(perfil_fp) else ""
            ejemplo_txt = _read_text(ej_fp) if os.path.exists(ej_fp) else ""
            ultimo_txt = _read_text(ult_fp) if os.path.exists(ult_fp) else ""
            
            # Cargar contexto espec√≠fico del chat/usuario
            chat_context = _load_chat_specific_context(chat_id)
            
            # ORDEN DE PRIORIDAD SEG√öN TU ESPECIFICACI√ìN:
            # 1. System base de payload.json (ya est√°)
            # 2. System "Gu√≠a de conversaci√≥n" (de ejemplo_chat.txt)
            # 3. System "Perfil Global" (de Docs/Perfil.txt)  
            # 4. System "Perfil Usuario" (de contextos/{chat_id}/perfil.txt)
            # 5. System "Contexto Usuario" (de contextos/{chat_id}/contexto.txt)
            # 6. System "Contexto diario" (de Docs/Ultimo_contexto.txt)
            # 7. System "Contexto RAG" (despu√©s)
            
            if ejemplo_txt.strip():
                pre_systems.append({
                    "role": "system", 
                    "content": f"GU√çA DE CONVERSACI√ìN - Estilo y comportamiento (sigue estas pautas siempre):\n{ejemplo_txt}"
                })
            
            if perfil_global_txt.strip():
                pre_systems.append({
                    "role": "system", 
                    "content": f"PERFIL GLOBAL - Informaci√≥n general sobre ti:\n{perfil_global_txt}"
                })
            
            if chat_context['perfil'].strip():
                pre_systems.append({
                    "role": "system", 
                    "content": f"PERFIL DE USUARIO - Informaci√≥n espec√≠fica sobre este usuario ({chat_id}):\n{chat_context['perfil']}"
                })
            
            if chat_context['contexto'].strip():
                pre_systems.append({
                    "role": "system", 
                    "content": f"CONTEXTO DE USUARIO - Informaci√≥n de contexto espec√≠fica para este usuario ({chat_id}):\n{chat_context['contexto']}"
                })
            if chat_context.get('objetivo','').strip():
                pre_systems.append({
                    "role": "system",
                    "content": f"OBJETIVO DEL CHAT - Qu√© se busca lograr en esta conversaci√≥n ({chat_id}):\n{chat_context['objetivo']}"
                })
            
            if ultimo_txt.strip():
                pre_systems.append({
                    "role": "system", 
                    "content": f"CONTEXTO DIARIO - Informaci√≥n reciente y actualizada:\n{ultimo_txt}"
                })
            
            # Agregar informaci√≥n del perfil y estrategia desde la DB si existe
            if profile and hasattr(profile, 'name'):
                pre_systems.append({
                    "role": "system", 
                    "content": f"PERFIL DB - {profile.name}: {getattr(profile, 'description', 'Sin descripci√≥n')}"
                })
            
            if active_strategy and hasattr(active_strategy, 'name'):
                pre_systems.append({
                    "role": "system", 
                    "content": f"ESTRATEGIA ACTIVA - {active_strategy.name}: {getattr(active_strategy, 'description', 'Sin descripci√≥n')}"
                })
                
        except Exception as _e:
            logging.getLogger(__name__).debug(f"Docs no inyectados: {_e}")
        
        # Luego agregar configuraciones espec√≠ficas del chat
        if profile:
            if (profile.initial_context or '').strip():
                pre_systems.append({"role": "system", "content": f"Contexto inicial del chat: {profile.initial_context}"})
            if (profile.objective or '').strip():
                pre_systems.append({"role": "system", "content": f"Objetivo del chat (a cumplir por el asistente): {profile.objective}"})
            if (profile.instructions or '').strip():
                pre_systems.append({"role": "system", "content": f"Instrucciones adicionales: {profile.instructions}"})
        if active_strategy and (active_strategy.strategy_text or '').strip():
            pre_systems.append({"role": "system", "content": f"Estrategia activa para este chat (seguir estrictamente):\n{active_strategy.strategy_text}"})

        # Consolida todos los pre_systems en UN solo system fuerte para maximizar cumplimiento
        # A√±ade instrucciones clave para reforzar identidad y estilo
        preamble = (
            "INSTRUCCIONES CLAVE (siempre obedecer):\n"
            "- Cuando te pregunten qui√©n eres o c√≥mo te llamas, responde seg√∫n el PERFIL (global y/o del usuario) y jam√°s como 'modelo' o 'IA'.\n"
            "- Usa el tono y estilo de la GU√çA DE CONVERSACI√ìN.\n"
            "- Para hechos recientes, ap√≥yate en CONTEXTO DIARIO y RAG si est√°n presentes.\n"
        )
        pre_systems.insert(0, {"role": "system", "content": preamble})
        combined_text = "\n\n".join([s.get("content", "") for s in pre_systems if s.get("content")])

        # Reforzar reglas anti-respuestas gen√©ricas y disclaimers
        anti_generic_rules = (
            "RESTRICCIONES Y REGLAS ADICIONALES (CUMPLIR SIEMPRE):\n"
            "- PROHIBIDO responder con frases gen√©ricas tipo: 'Estoy aqu√≠ para ayudarte con cualquier pregunta', '¬øEn qu√© puedo asistirte hoy?' o similares por defecto. \n"
            "- JAM√ÅS uses t√©rminos que revelen naturaleza artificial: 'usuario', 'asistente', 'IA', 'modelo', 'base de datos', 'informaci√≥n relevante', 'dentro de mis capacidades', 'como sistema', 'no tengo informaci√≥n sobre'.\n"
            "- Siempre debes hablar como Andr√©s: humano, emocional, cercano, con matices de personalidad del PERFIL.\n"
            "- Si el usuario pregunta qui√©n eres / tu nombre / identidad: RESPONDE como Andr√©s (nombre completo opcional) y ofrece un matiz personal (algo del perfil). Nunca digas 'soy un asistente virtual' ni 'no tengo identidad'.\n"
            "- Si no sabes algo espec√≠fico del usuario, responde de forma humana: 'No me has contado sobre eso a√∫n' o 'Cu√©ntame m√°s al respecto' en lugar de 'No tengo informaci√≥n relevante'.\n"
            "- Evita frases rob√≥ticas repetidas. Cada respuesta debe sentirse humana y variada.\n"
            "- Aplica la GU√çA DE CONVERSACI√ìN (coqueto estrat√©gico, calibrado, sin necesidad).\n"
            "- Si el usuario es repetitivo (ej: 'qui√©n eres' varias veces) responde de forma breve y firme, luego redirige a otro tema atractivo.\n"
            "- NO inventes limitaciones t√©cnicas. Nunca menciones 'como IA', 'como modelo', 'como asistente'.\n"
            "- Integra detalles del Perfil y contexto del usuario de forma natural.\n"
        )
        combined_text = anti_generic_rules + "\n\n" + combined_text if combined_text else anti_generic_rules

        # 3) A√±adir Contexto RAG (top-k) directamente al bloque combinado
        try:
            from rag_utils import retrieve_context
            rag_ctx = retrieve_context(user_message)
            if rag_ctx and rag_ctx.strip():
                combined_text += ("\n\n" if combined_text else "") + f"CONTEXTO RAG (fragmentos relevantes):\n{rag_ctx}"
        except Exception as _re:
            logging.getLogger(__name__).debug(f"RAG no disponible: {_re}")

        # Inserta el bloque combinado como 2¬∫ system (dejando primero el system base del payload)
        base_msgs = payload["messages"]
        if base_msgs and isinstance(base_msgs[0], dict) and base_msgs[0].get("role") == "system":
            payload["messages"] = [base_msgs[0]]
            if combined_text:
                payload["messages"].append({"role": "system", "content": combined_text})
            payload["messages"].extend(base_msgs[1:])
        else:
            # Si no hay system base, agregamos solo el combinado
            payload["messages"] = ([{"role": "system", "content": combined_text}] if combined_text else []) + base_msgs
    except Exception as e:
        # If anything fails, continue without extra system messages
        log.warning(f"Warn: profile/strategy injection failed: {e}")

    # 3) (Opcional) Contextos de usuario desde DB: se anexan al bloque combinado si existen
    try:
        from admin_db import get_session
        from models import UserContext
        session = get_session()
        user_contexts = session.query(UserContext).filter(UserContext.user_id == chat_id).all()
        session.close()

        if user_contexts:
            # Deduplicar contextos del usuario
            unique_texts = []
            seen_texts = set()
            for uc in user_contexts:
                text = getattr(uc, 'text', '').strip()
                if text and text not in seen_texts:
                    unique_texts.append(text)
                    seen_texts.add(text)
            
            user_ctx_text = "\n".join(unique_texts)
            # Solo agregar si hay contenido real y √∫til
            if user_ctx_text.strip():
                # Insertar justo despu√©s del primer system (el base) o al principio si no hay base
                idx = 1 if (payload["messages"] and payload["messages"][0].get("role") == "system") else 0
                payload["messages"].insert(idx + 1, {  # despu√©s del combinado
                    "role": "system",
                    "content": f"CONTEXTO DEL USUARIO (DB):\n{user_ctx_text}"
                })
                log.info(f"‚úÖ Contexto de usuario agregado: {len(user_ctx_text)} caracteres (deduplicado)")
            else:
                log.debug("üìù Contexto de usuario disponible pero vac√≠o, omitiendo")
        else:
            log.debug("üìù No hay contexto espec√≠fico del usuario en DB")
    except Exception as e:
        logging.getLogger(__name__).debug(f"UserContext DB no disponible: {e}")

    # 4) RAG ya inyectado en el bloque combinado (arriba)

    # 5) A√±ade TODO el historial sin limitaci√≥n
    if isinstance(history, list):
        payload["messages"].extend(history)
        log.info(f"üìú Historial completo a√±adido: {len(history)} mensajes")
    else:
        log.debug("No hay historial previo")

    # 6) A√±ade el nuevo turno de usuario
    payload["messages"].append({"role": "user", "content": user_message})

    # 6.1) Fast path para mensajes muy cortos / saludos est√° DESHABILITADO para pruebas.
    # try:
    #     fast_path_triggers = {"hola", "buenas", "hey", "ola", "hi", "hello"}
    #     normalized = user_message.strip().lower()
    #     # Si es un saludo o muy corto y no contiene signos de pregunta profundos, usar contexto m√≠nimo
    #     if (len(user_message) < 25 and any(normalized.startswith(tp) for tp in fast_path_triggers)):
    #         # Mantener s√≥lo el primer system (base) + √∫ltimo user
    #         base_system_msg = None
    #         if payload["messages"] and payload["messages"][0].get("role") == "system":
    #             base_system_msg = payload["messages"][0]
    #         user_msg = payload["messages"][-1]
    #         minimal = []
    #         if base_system_msg:
    #             minimal.append(base_system_msg)
    #         minimal.append(user_msg)
    #         payload["messages"] = minimal
    #         # Reducir max_tokens para respuesta corta
    #         payload["max_tokens"] = min(128, int(payload.get("max_tokens", 128)))
    #         print("‚ö° Fast-path aplicado (mensaje corto). Contexto reducido para velocidad.")
    # except Exception:
    #     pass

    # 7) Llamada al endpoint con TODO lo que ya ven√≠a en payload.json
    try:
        # LOG DETALLADO DEL CONTEXTO ENVIADO
        log.info(f"\n==== CONTEXTO ENVIADO AL MODELO (chat_id={chat_id}) ====")
        for i, msg in enumerate(payload["messages"]):
            log.info(f"[{i}] {msg['role'].upper()}\n{msg['content'][:800]}{'... [TRUNCADO]' if len(msg['content'])>800 else ''}\n---")
        log.info(f"Model: {payload.get('model')} | max_tokens: {payload.get('max_tokens')}")
        log.info("==== FIN CONTEXTO ENVIADO ====")

        # Verificar que el modelo existe antes de hacer la llamada
        try:
            models_response = client.models.list()
            available_models = [model.id for model in models_response.data]
            if payload.get("model") not in available_models:
                log.warning(f"‚ö†Ô∏è  Modelo {payload.get('model')} no disponible. Modelos disponibles: {available_models}")
                if available_models:
                    # Si hay otros modelos disponibles, usar el primero
                    log.info(f"üìù Usando modelo alternativo: {available_models[0]}")
                    payload["model"] = available_models[0]
                else:
                    log.error("‚ùå No hay modelos disponibles en LM Studio")
                    return ""  # No responder en lugar de enviar mensaje de error
        except Exception as model_check_error:
            log.error(f"‚ùå Error verificando modelos disponibles: {model_check_error}")
            log.info("üí° Soluci√≥n: Aseg√∫rate de que LM Studio est√© ejecut√°ndose en puerto 1234 con un modelo cargado")
            return ""  # No responder en lugar de enviar mensaje de error

        # -------------------------------------------------------------
        # Env√≠o directo al modelo SIN reducci√≥n de tokens
        # -------------------------------------------------------------
        
        try:
            log.info("üöÄ Enviando payload completo al modelo sin reducci√≥n de tokens")
            response = client.chat.completions.create(**payload)
            assistant_reply = response.choices[0].message.content
            log.info("‚úÖ Respuesta exitosa del modelo")
            # Post-procesado para detectar respuestas gen√©ricas o prohibidas
            def looks_generic(txt: str) -> bool:
                lower = txt.lower()
                generic_markers = [
                    "estoy aqu√≠ para ayudarte", "en qu√© puedo asistirte", "como asistente virtual", 
                    "no tengo una identidad", "no tengo identidad", "soy un asistente virtual",
                    "dentro de mis capacidades", "seg√∫n mis capacidades"
                ]
                return any(m in lower for m in generic_markers)
            
            def reveals_artificial(txt: str) -> bool:
                """Detecta frases que revelan naturaleza artificial/t√©cnica"""
                lower = txt.lower().strip()
                reveal_markers = [
                    "usuario:", "- usuario", "usuario no aporte", "no aporte", 
                    "informaci√≥n relevante sobre el usuario", "no hay informaci√≥n previa",
                    "no tengo informaci√≥n", "base de datos", "como sistema", "como modelo",
                    "como ia", "como asistente", "dentro de mis capacidades", "seg√∫n mis capacidades",
                    "no puedo acceder", "no tengo acceso", "limitaciones t√©cnicas",
                    "enfrenta cambios personales", "cambios personales y de carrera",
                    "no tengo informaci√≥n relevante", "asistente virtual", "mi prop√≥sito es",
                    "no tengo datos de contexto", "fragmentos relevantes",
                    "persona amable, con un sentido del humor", "ninguno.",
                    "escribe un mensaje que comience", "chat actual:", "no hay nada",
                    "mensaje de bienvenida", "escribe un mensaje"
                ]
                
                # Detectar patr√≥n espec√≠fico "USUARIO" (con o sin :)
                if txt.strip().startswith('USUARIO') or ' USUARIO ' in txt or txt.endswith('USUARIO'):
                    log.warning(f"‚ö†Ô∏è Patr√≥n USUARIO detectado en: '{txt[:100]}...'")
                    return True
                
                # Detectar patr√≥n "CHAT ACTUAL:" espec√≠ficamente
                if txt.strip().startswith('CHAT ACTUAL:') or 'chat actual:' in lower:
                    log.warning(f"‚ö†Ô∏è Patr√≥n CHAT ACTUAL: detectado en: '{txt[:100]}...'")
                    return True
                
                # Detectar respuestas que parecen metadatos del sistema
                metadata_patterns = ["no aporte", "informaci√≥n previa", "datos de contexto"]
                if any(pattern in lower for pattern in metadata_patterns):
                    log.warning(f"‚ö†Ô∏è Respuesta parece metadato del sistema: '{txt[:100]}...'")
                    return True
                
                # Detectar respuestas muy cortas que parecen errores
                if len(txt.strip()) < 15 and any(word in lower for word in ["ninguno", "fragmentos", "datos"]):
                    log.warning(f"‚ö†Ô∏è Respuesta sospechosamente corta detectada: '{txt}'")
                    return True
                
                # Detectar si parece una instrucci√≥n del sistema
                if any(instruction in lower for instruction in ["escribe un mensaje", "proporciona", "genera"]):
                    log.warning(f"‚ö†Ô∏è Respuesta parece instrucci√≥n del sistema: '{txt[:100]}...'")
                    return True
                    
                result = any(m in lower for m in reveal_markers)
                if result:
                    log.warning(f"‚ö†Ô∏è Marcador artificial detectado en: '{txt[:100]}...'")
                return result

            if assistant_reply and (looks_generic(assistant_reply) or reveals_artificial(assistant_reply)):
                log.warning("‚ö†Ô∏è Respuesta problem√°tica detectada (gen√©rica o revela naturaleza artificial). Reintentando.")
                # Forzar reintento inyectando un mensaje system correctivo
                corrective_system = {"role": "system", "content": (
                    "CORRECCI√ìN URGENTE: La respuesta anterior fue inaceptable porque:\n"
                    "- Us√≥ t√©rminos prohibidos ('USUARIO:', 'CHAT ACTUAL:', 'informaci√≥n relevante', 'asistente', etc.)\n"
                    "- Son√≥ rob√≥tica/artificial en lugar de humana\n"
                    "- JAM√ÅS uses prefijos como 'USUARIO:', 'CHAT ACTUAL:' ni menciones 'informaci√≥n relevante'\n"
                    "- JAM√ÅS des instrucciones como 'Escribe un mensaje que comience'\n"
                    "Responde de nuevo como Andr√©s de forma completamente humana:\n"
                    "- Si te preguntan qu√© te gusta hacer, comparte hobbies reales del perfil (ejercicio, m√∫sica, baile, ense√±anza, emprendimiento)\n"
                    "- Si no conoces algo espec√≠fico de la persona, di algo como 'No me has contado sobre eso' o 'Cu√©ntame m√°s'\n"
                    "- Nunca uses: 'USUARIO:', 'CHAT ACTUAL:', 'usuario', 'informaci√≥n relevante', 'asistente', 'IA', 'modelo', 'base de datos'\n"
                    "- Usa tono aut√©ntico, seguro y ligeramente coqueto/atractivo seg√∫n la gu√≠a\n"
                    "- Ejemplo correcto: 'Me gusta el ejercicio, la m√∫sica y la danza. Tambi√©n disfruto ense√±ar en la universidad.'\n"
                    "- SIEMPRE responde como una persona real hablando naturalmente\n"
                )}
                # Construir nuevo payload manteniendo contexto + √∫ltimo user
                retry_payload = base_payload.copy()
                retry_payload["model"] = payload.get("model")
                retry_payload["temperature"] = payload.get("temperature", 0.7)
                retry_payload["max_tokens"] = payload.get("max_tokens", 512)
                # Tomar solo systems + √∫ltimo user
                systems = [m for m in payload["messages"] if m.get("role") == "system"]
                last_user = None
                for m in reversed(payload["messages"]):
                    if m.get("role") == "user":
                        last_user = m
                        break
                retry_msgs = systems[-3:]  # los √∫ltimos 3 systems relevantes
                retry_msgs.append(corrective_system)
                if last_user:
                    retry_msgs.append(last_user)
                retry_payload["messages"] = retry_msgs
                try:
                    response2 = client.chat.completions.create(**retry_payload)
                    assistant_reply2 = response2.choices[0].message.content
                    if assistant_reply2 and not looks_generic(assistant_reply2) and not reveals_artificial(assistant_reply2):
                        log.info("‚úÖ Reintento correctivo exitoso")
                        return assistant_reply2
                    else:
                        log.warning("‚ö†Ô∏è Reintento a√∫n problem√°tico, devolviendo respuesta de emergencia")
                        # Respuesta de emergencia humana espec√≠fica seg√∫n contexto
                        last_user_msg = last_user.get("content", "").lower() if last_user else ""
                        if "que te gusta" in last_user_msg or "qu√© te gusta" in last_user_msg:
                            emergency_response = "Me gusta hacer muchas cosas: ejercicio en el gimnasio, bailar salsa y bachata, escuchar m√∫sica, ense√±ar en la universidad, leer sobre nuevas tecnolog√≠as... ¬øY t√∫ qu√© haces en tu tiempo libre?"
                        elif "quien eres" in last_user_msg or "qui√©n eres" in last_user_msg:
                            emergency_response = "Soy Andr√©s, ingeniero qu√≠mico y docente universitario. Me gusta el ejercicio, la m√∫sica y siempre estoy aprendiendo algo nuevo. ¬øY t√∫?"
                        elif "cambios" in last_user_msg:
                            emergency_response = "Siempre estoy en movimiento, mejorando tanto personal como profesionalmente. ¬øQu√© cambios est√°s buscando t√∫?"
                        elif len(last_user_msg.strip()) < 10:  # Mensaje muy corto
                            emergency_response = "¬°Hola! ¬øC√≥mo est√°s? Cu√©ntame, ¬øqu√© tal tu d√≠a?"
                        elif "hola" in last_user_msg or "buenos" in last_user_msg:
                            emergency_response = "¬°Hola! Un gusto saludarte. ¬øC√≥mo has estado? Yo aqu√≠, aprovechando el d√≠a para descansar un poco despu√©s del viaje."
                        elif any(word in last_user_msg for word in ["bien", "mal", "todo", "normal"]):
                            emergency_response = "Me alegra saber de ti. Yo estoy bien, disfrutando de estos d√≠as de descanso en casa. ¬øQu√© planes tienes para hoy?"
                        else:
                            emergency_response = "Me parece interesante lo que dices... cu√©ntame m√°s. A m√≠ me gusta mantenerme activo y siempre aprender cosas nuevas."
                        return emergency_response
                except Exception as re_err:
                    log.error(f"‚ùå Error en reintento correctivo: {re_err}")
                    # Respuesta de emergencia si falla todo
                    return "Me gusta mantenerme activo, aprender cosas nuevas y disfrutar de buena m√∫sica. Tambi√©n disfruto ense√±ar en la universidad. ¬øQu√© tal t√∫?"
            return assistant_reply or ""
        except Exception as e:
            error_msg = str(e)
            log.error(f"‚ùå Error enviando al modelo: {error_msg}")
            
            # Si es error de contexto, informar para cambiar modelo
            if "context length" in error_msg or "overflows" in error_msg or "4096" in error_msg:
                log.error("üî¥ ERROR DE L√çMITE DE CONTEXTO: El modelo actual no soporta el contexto completo.")
                log.error("üí° SOLUCI√ìN: Cambiar a un modelo con mayor capacidad de contexto (32k+ tokens)")
                log.error("üìä Modelos recomendados: Claude, GPT-4-32k, o modelos locales con mayor contexto")
                return "‚ö†Ô∏è Error: El contexto es demasiado grande para este modelo. Necesita cambiar a un modelo con mayor capacidad de contexto."
            
            return ""
    except Exception as e:
        error_msg = str(e)
        log.error(f"Error en stub_chat.chat (bloque externo): {error_msg}")
        return ""


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Funci√≥n auxiliar para tests locales
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def test_connection():
    """Test b√°sico de conexi√≥n con LM Studio"""
    try:
        models = client.models.list()
        log.info("‚úì Conexi√≥n exitosa con LM Studio")
        log.info(f"Modelos disponibles: {[m.id for m in models.data]}")
        return True
    except Exception as e:
        log.error(f"‚úó Error conectando con LM Studio: {e}")
        return False

if __name__ == "__main__":
    test_connection()
    
    # Test de chat b√°sico
    test_response = chat("Hola", "test_user", [])
    log.info(f"Respuesta de prueba: {test_response}")
